{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279b0dae-2de1-45c7-b8fb-dcde93f7c7f2",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Prepare and transform data in the Lakehouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c20b5fa-9b45-4555-9cae-32f231a39679",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-02T15:10:45.2531901Z",
       "execution_start_time": "2025-06-02T15:10:44.9456154Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "54482a9a-19cc-4e7e-87e6-910516f701c5",
       "queued_time": "2025-06-02T15:10:44.9444183Z",
       "session_id": "6a6c6c0d-6a3b-46c0-871d-509544a1a67b",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, 6a6c6c0d-6a3b-46c0-871d-509544a1a67b, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2679c38e-c7da-47dc-9745-63a8dfd91253",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-02T15:10:47.6487312Z",
       "execution_start_time": "2025-06-02T15:10:45.2554649Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b73acf86-a0d1-45fe-af1e-e3477253e203",
       "queued_time": "2025-06-02T15:10:45.0944181Z",
       "session_id": "6a6c6c0d-6a3b-46c0-871d-509544a1a67b",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, 6a6c6c0d-6a3b-46c0-871d-509544a1a67b, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Row(SaleKey=7250401, CityKey=64141, CustomerKey=0, BillToCustomerKey=0, StockItemKey=139, InvoiceDateKey=datetime.datetime(2000, 2, 18, 0, 0), DeliveryDateKey=datetime.datetime(2000, 2, 19, 0, 0), SalespersonKey=49, WWIInvoiceID=2999, Description='\"The Gu\" red shirt XML tag t-shirt (White) L:\"The Gu\" red shirt XML tag t-shirt (White) L:\"The Gu\" red shirt XML tag t-shirt (White) L:\"The Gu\" red shirt XML tag t-shirt (White) L:\"The Gu\" red shirt XML tag t-shirt (White) L:\"The Gu\" red shirt XML tag t-shirt (White) L:\"The Gu\" red shirt XML tag t-shirt (White) L:\"The Gu\" red shirt XML tag t-shirt (White) L:\"The Gu\" red shirt XML tag t-shirt (White) L:\"The Gu\" red shirt XML tag t-shirt (White) L:\"The Gu\" red shirt XML tag t-shirt (White) L:\"The Gu\" red shirt XML tag t-shirt (White) L:', Package='Each', Quantity=48, UnitPrice=Decimal('18.00'), TaxRate=Decimal('15.000'), TotalExcludingTax=Decimal('864.00'), TaxAmount=Decimal('129.60'), Profit=Decimal('528.00'), TotalIncludingTax=Decimal('993.60'), TotalDryItems=48, TotalChillerItems=0, LineageKey=11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data as a dataframe\n",
    "df = spark.read.format(\"parquet\").load('Files/imported_files/WideWorldImportersDW/parquet/full/fact_sale_1y_full')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a0c4b-9dbb-48e3-8f24-9dbaaa2ba829",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-02T15:11:51.4562829Z",
       "execution_start_time": "2025-06-02T15:10:47.6510955Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f1ede750-ad96-4add-af09-142a4b478b4c",
       "queued_time": "2025-06-02T15:10:45.375577Z",
       "session_id": "6a6c6c0d-6a3b-46c0-871d-509544a1a67b",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, 6a6c6c0d-6a3b-46c0-871d-509544a1a67b, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year, month, quarter\n",
    "\n",
    "# Load the raw fact_sale data from the parquet file into a DataFrame.\n",
    "fact_sale_raw_df = spark.read.format(\"parquet\").load('Files/imported_files/WideWorldImportersDW/parquet/full/fact_sale_1y_full')\n",
    "\n",
    "# --- Feature Engineering: Add Date-Based Columns ---\n",
    "# To support time-based analysis, new columns for Year, Quarter, and Month\n",
    "# are derived from the 'InvoiceDateKey'. Chaining .withColumn() is an efficient\n",
    "# way to apply multiple transformations in sequence.\n",
    "fact_sale_transformed_df = fact_sale_raw_df.withColumn(\"SaleYear\", year(col(\"InvoiceDateKey\"))) \\\n",
    "                                           .withColumn(\"SaleQuarter\", quarter(col(\"InvoiceDateKey\"))) \\\n",
    "                                           .withColumn(\"SaleMonth\", month(col(\"InvoiceDateKey\")))\n",
    "\n",
    "# --- Verification Step ---\n",
    "# Before writing the data, it's good practice to inspect the new schema\n",
    "# to ensure the transformations were applied correctly.\n",
    "print(\"Schema of the transformed fact_sale table:\")\n",
    "fact_sale_transformed_df.printSchema()\n",
    "\n",
    "# --- Write to Delta Table ---\n",
    "# Save the transformed DataFrame as a partitioned Delta table.\n",
    "# Partitioning by Year and Quarter will significantly speed up queries\n",
    "# that filter on these date ranges.\n",
    "OUTPUT_TABLE_NAME = \"fact_sale\"\n",
    "fact_sale_transformed_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .partitionBy(\"SaleYear\", \"SaleQuarter\") \\\n",
    "    .save(f\"Tables/{OUTPUT_TABLE_NAME}\")\n",
    "\n",
    "print(f\"\\nSuccessfully created and saved the partitioned '{OUTPUT_TABLE_NAME}' table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5cf4a4-4145-425c-832d-0613fc0e6016",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-02T15:11:51.7684276Z",
       "execution_start_time": "2025-06-02T15:11:51.4587066Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "7a000a13-6dda-482e-87e9-18606329ba94",
       "queued_time": "2025-06-02T15:10:45.7645108Z",
       "session_id": "6a6c6c0d-6a3b-46c0-871d-509544a1a67b",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, 6a6c6c0d-6a3b-46c0-871d-509544a1a67b, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def process_and_load_dimension(table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Reads a raw dimension table from a parquet file, removes the 'Photo' column,\n",
    "    and saves the result as a clean Delta table in the Lakehouse.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): The name of the dimension table to process.\n",
    "                          This name is used for both the source file and the target table.\n",
    "    \"\"\"\n",
    "    print(f\"--> Starting processing for dimension: '{table_name}'\")\n",
    "    \n",
    "    # Define paths\n",
    "    source_path = f\"Files/imported_files/WideWorldImportersDW/parquet/full/{table_name}\"\n",
    "    target_table = f\"Tables/{table_name}\"\n",
    "    \n",
    "    # Read the raw data\n",
    "    dimension_df = spark.read.format(\"parquet\").load(source_path)\n",
    "    \n",
    "    # Remove the 'Photo' column as it is not needed for analysis\n",
    "    cleaned_dimension_df = dimension_df.select([c for c in dimension_df.columns if c != 'Photo'])\n",
    "    \n",
    "    # Write the cleaned DataFrame to a Delta table\n",
    "    cleaned_dimension_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .save(target_table)\n",
    "        \n",
    "    print(f\"--- Successfully saved table: '{table_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a80954-53a8-4ea3-a634-e82b920958a6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-02T15:12:16.843058Z",
       "execution_start_time": "2025-06-02T15:11:51.7704287Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "033a9268-dcc1-41a4-800a-f1c5c5d8616a",
       "queued_time": "2025-06-02T15:10:46.068273Z",
       "session_id": "6a6c6c0d-6a3b-46c0-871d-509544a1a67b",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 6a6c6c0d-6a3b-46c0-871d-509544a1a67b, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for 'dimension_city'...\n",
      "--- Done loading 'dimension_city'.\n",
      "Loading data for 'dimension_customer'...\n",
      "--- Done loading 'dimension_customer'.\n",
      "Loading data for 'dimension_date'...\n",
      "--- Done loading 'dimension_date'.\n",
      "Loading data for 'dimension_employee'...\n",
      "--- Done loading 'dimension_employee'.\n",
      "Loading data for 'dimension_stock_item'...\n",
      "--- Done loading 'dimension_stock_item'.\n",
      "\n",
      "All dimension tables have been loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration: Define Dimension Tables ---\n",
    "# Create a list of all dimension tables that need to be processed.\n",
    "# This list will be iterated through, calling the processing function for each table.\n",
    "DIMENSION_TABLES_TO_PROCESS = [\n",
    "  'dimension_city',\n",
    "  'dimension_customer',\n",
    "  'dimension_date',\n",
    "  'dimension_employee',\n",
    "  'dimension_stock_item'\n",
    "]\n",
    "\n",
    "# --- Execution: Loop and Process Each Dimension ---\n",
    "# Iterate through the list and call our custom function for each table name.\n",
    "# The logging/print statements are now inside the function itself,\n",
    "# which keeps this loop clean and focused on its main job: orchestration.\n",
    "for dim_table in DIMENSION_TABLES_TO_PROCESS:\n",
    "    process_and_load_dimension(dim_table)\n",
    "\n",
    "print(\"\\n All dimension tables have been successfully processed and loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fc868-23c5-485d-bee6-80eec2fa9da5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Data Transformation - Business Aggregates (PySpark Approach)\n",
    "The first business aggregation we need is a summary of sales totals for each city, by day. This will be created by joining the `fact_sale` table with the `dimension_date` and `dimension_city` tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bef195-caaf-4753-aa7d-85fc7923c224",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-02T15:12:35.0200158Z",
       "execution_start_time": "2025-06-02T15:12:16.8460193Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "5cb135be-f3c2-48e4-9826-9a4cc82c5feb",
       "queued_time": "2025-06-02T15:10:46.2459146Z",
       "session_id": "6a6c6c0d-6a3b-46c0-871d-509544a1a67b",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, 6a6c6c0d-6a3b-46c0-871d-509544a1a67b, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the three tables into DataFrames.\n"
     ]
    }
   ],
   "source": [
    "# Load the necessary tables from the Lakehouse\n",
    "fact_sale_df = spark.read.table(\"medium1.fact_sale\")\n",
    "dim_date_df = spark.read.table(\"medium1.dimension_date\")\n",
    "dim_city_df = spark.read.table(\"medium1.dimension_city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5ad86-2adc-4654-b632-c179844da45e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-06-02T15:12:35.8532237Z",
       "execution_start_time": "2025-06-02T15:12:35.0224023Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "cb48711f-2872-4142-afef-39fd6c5ecfa5",
       "queued_time": "2025-06-02T15:10:46.4048543Z",
       "session_id": "6a6c6c0d-6a3b-46c0-871d-509544a1a67b",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, 6a6c6c0d-6a3b-46c0-871d-509544a1a67b, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation complete. The aggregated data is now in the 'sale_by_date_city' DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Join the fact table with its dimensions ---\n",
    "# This brings all the necessary descriptive information together with the sales numbers.\n",
    "joined_df = fact_sale_df.alias(\"sales\") \\\n",
    "    .join(dim_date_df.alias(\"date\"), col(\"sales.InvoiceDateKey\") == col(\"date.Date\"), \"inner\") \\\n",
    "    .join(dim_city_df.alias(\"city\"), col(\"sales.CityKey\") == col(\"city.CityKey\"), \"inner\")\n",
    "\n",
    "# Optional: You can display this intermediate DataFrame to verify the join worked\n",
    "# display(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591a7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Group by date/city and calculate sums ---\n",
    "# This performs the main aggregation to get our business metrics.\n",
    "city_sales_agg_df = joined_df \\\n",
    "    .groupBy(\"date.Date\", \"date.CalendarMonthLabel\", \"city.City\", \"city.StateProvince\") \\\n",
    "    .sum(\"sales.TotalIncludingTax\", \"sales.Profit\") \\\n",
    "    .withColumnRenamed(\"sum(TotalIncludingTax)\", \"TotalSales\") \\\n",
    "    .withColumnRenamed(\"sum(Profit)\", \"TotalProfit\") \\\n",
    "    .orderBy(\"date.Date\")\n",
    "\n",
    "# --- Step 3: Display the final result before saving ---\n",
    "# This verification step lets us see our final summary table.\n",
    "print(\"Displaying final aggregated data:\")\n",
    "display(city_sales_agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c95b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Save the final aggregated data as a new table ---\n",
    "city_sales_agg_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(\"Tables/aggregate_sale_by_date_city\")\n",
    "\n",
    "print(\"Successfully saved the 'aggregate_sale_by_date_city' table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d7395",
   "metadata": {},
   "source": [
    "Create Employee-Level Sales Aggregate (Spark SQL Approach)\n",
    "As an alternative approach, we can use Spark SQL to answer a different business question: \"What are the total sales for each employee on each day?\". This demonstrates the flexibility of using SQL for complex aggregations.\n",
    "\n",
    "First, a temporary view is created to define the query logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- This view joins sales with employee and date dimensions to aggregate sales data at the employee level.\n",
    "-- Using a temporary view is a good practice as it separates the complex query logic from the final save operation.\n",
    "\n",
    "CREATE OR REPLACE TEMPORARY VIEW view_employee_sales_by_date\n",
    "AS\n",
    "SELECT \n",
    "    -- Date and Employee Attributes for grouping\n",
    "    DD.Date,\n",
    "    DD.CalendarMonthLabel,\n",
    "    DE.Employee,\n",
    "    DE.PreferredName,\n",
    "    \n",
    "    -- Aggregated Metrics\n",
    "    SUM(FS.TotalIncludingTax) AS TotalSales,\n",
    "    SUM(FS.Profit) AS TotalProfit\n",
    "\n",
    "-- Define table sources and aliases\n",
    "FROM medium1.fact_sale AS FS\n",
    "-- Join Logic\n",
    "INNER JOIN medium1.dimension_date AS DD ON FS.InvoiceDateKey = DD.Date\n",
    "INNER JOIN medium1.dimension_employee AS DE ON FS.SalespersonKey = DE.EmployeeKey\n",
    "\n",
    "-- Grouping by all non-aggregated columns\n",
    "GROUP BY\n",
    "    DD.Date,\n",
    "    DD.CalendarMonthLabel,\n",
    "    DE.Employee,\n",
    "    DE.PreferredName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86737a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Materialize the View ---\n",
    "# The SQL cell only defined the view; this command executes the query\n",
    "# and loads the results into a DataFrame.\n",
    "employee_sales_agg_df = spark.sql(\"SELECT * FROM view_employee_sales_by_date ORDER BY Date, PreferredName\")\n",
    "\n",
    "\n",
    "# --- Step 2: Save the final aggregated data as a new table ---\n",
    "employee_sales_agg_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(\"Tables/aggregate_sale_by_date_employee\")\n",
    "\n",
    "print(\"Successfully saved the 'aggregate_sale_by_date_employee' table.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "1d62eec1-4ae3-45d3-85c6-befa936564dc",
    "default_lakehouse_name": "medium1",
    "default_lakehouse_workspace_id": "d5e083f9-01a0-4b3f-a56c-f6863c0382a3",
    "known_lakehouses": [
     {
      "id": "1d62eec1-4ae3-45d3-85c6-befa936564dc"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
